{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e58f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from config import Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22070ad0",
   "metadata": {},
   "source": [
    "# Reading The Data\n",
    "\n",
    "* base_path is the base path for all the code, datasets, and models\n",
    "* the data is read using json library from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe811711",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"MSRVTT/raw-data/train_val_videodatainfo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5da09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.load(open(base_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c7b2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = json_data['sentences']\n",
    "videos = json_data['videos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15478245",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WATCHOUTT\n",
    "\n",
    "num_videos = 1000 # The number of videos you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c1fee",
   "metadata": {},
   "source": [
    "## Storing The Data\n",
    "\n",
    "1. Get all the needed video id for training\n",
    "2. Get all the needed captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e558886",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = {'train': [], 'validate': [], 'test': []}\n",
    "count = 0\n",
    "\n",
    "num_train = 850\n",
    "num_val = 50\n",
    "num_test = 100\n",
    "\n",
    "for v in videos:\n",
    "    \n",
    "    # Video Id that will be used for training collected\n",
    "    # here\n",
    "    if count < num_train:\n",
    "        split['train'].append(int(v['id']))\n",
    "        count += 1\n",
    "        \n",
    "    elif count >= num_train and count < num_train + num_val:\n",
    "        split['validate'].append(int(v['id']))\n",
    "        count += 1\n",
    "        \n",
    "    elif count >= num_train + num_val and count < num_train + num_val + num_test:\n",
    "        split['test'].append(int(v['id']))\n",
    "        count += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a73ea1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the len of train split: 850\n",
      "With the first index :[0, 1, 2, 3, 4] and the last index :[845, 846, 847, 848, 849]\n",
      "\n",
      "This is the len of validation split: 50\n",
      "With the first index :[850, 851, 852, 853, 854] and the last index :[895, 896, 897, 898, 899]\n",
      "\n",
      "This is the len of test split: 100\n",
      "With the first index :[900, 901, 902, 903, 904] and the last index :[995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the len of train split: \" + str(len(split[\"train\"])))\n",
    "print(\"With the first index :\" + str(split[\"train\"][:5]) + \" and the last index :\" + str(split[\"train\"][-5:]))\n",
    "print()\n",
    "print(\"This is the len of validation split: \" + str(len(split[\"validate\"])))\n",
    "print(\"With the first index :\" + str(split[\"validate\"][:5]) + \" and the last index :\" + str(split[\"validate\"][-5:]))\n",
    "print()\n",
    "print(\"This is the len of test split: \" + str(len(split[\"test\"])))\n",
    "print(\"With the first index :\" + str(split[\"test\"][:5]) + \" and the last index :\" + str(split[\"test\"][-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8034d4",
   "metadata": {},
   "source": [
    "## Get Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8f3e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_caps_all = defaultdict(list)\n",
    "raw_caps_train = defaultdict(list)\n",
    "references = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "31570bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 140200/140200 [00:00<00:00, 293120.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(sentences):\n",
    "    if int(item['video_id'][5:]) < num_videos:\n",
    "        vid = item['video_id']\n",
    "        tokens = [token.lower() for token in item['caption'].split() if token not in string.punctuation]\n",
    "\n",
    "        raw_caps_all[vid].append(tokens)\n",
    "\n",
    "        if int(vid[5:]) in split['train']:\n",
    "            raw_caps_train[vid].append(tokens)\n",
    "\n",
    "        references[vid].append({\n",
    "            'image_id': vid, \n",
    "            'cap_id': len(references[vid]), \n",
    "            'caption': ' '.join(tokens)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc6a9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "itoc = {}\n",
    "split_category = {'train': defaultdict(list), 'validate': defaultdict(list), 'test': defaultdict(list)}\n",
    "count = 0\n",
    "\n",
    "for item in videos:\n",
    "    ## Check for the len of video to train\n",
    "    if count < num_videos:\n",
    "        itoc[item['id']] = item['category']\n",
    "        split_category[item['split']][int(item[\"category\"])].append(int(item['id']))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ddb4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'split': split, \n",
    "    'raw_caps_train': raw_caps_train, \n",
    "    'raw_caps_all': raw_caps_all, \n",
    "    'references': references,\n",
    "    'itoc': itoc,\n",
    "    'split_category': split_category\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cf844933",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = results['split']\n",
    "raw_caps_train = results['raw_caps_train']\n",
    "raw_caps_all = results['raw_caps_all']\n",
    "references = results.get('references', None)\n",
    "\n",
    "vid2id = results.get('vid2id', None)\n",
    "itoc = results.get('itoc', None)\n",
    "split_category = results.get('split_category', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de41a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(train_vid2caps, count_thr, sort_vocab=False):\n",
    "    # count up the number of words\n",
    "    counts = {}\n",
    "    for vid, caps in train_vid2caps.items():\n",
    "        for cap in caps:\n",
    "            for w in cap:\n",
    "                counts[w] = counts.get(w, 0) + 1\n",
    "\n",
    "    bad_words = [w for w, n in counts.items() if n <= count_thr]\n",
    "    bad_count = sum(counts[w] for w in bad_words)\n",
    "    total_words = sum(counts.values())\n",
    "\n",
    "    print('- The number of bad words: %d/%d = %.2f%%' %\n",
    "          (len(bad_words), len(counts), len(bad_words) * 100.0 / len(counts)))\n",
    "    print('- The number of the vocabulary: %d' % (len(counts) - len(bad_words)))\n",
    "    print('- The number of UNKs: %d/%d = %.2f%%' %\n",
    "          (bad_count, total_words, bad_count * 100.0 / total_words))\n",
    "\n",
    "    candidate_vocab = [(w, n) for w, n in counts.items() if n > count_thr]\n",
    "    if sort_vocab:\n",
    "        print('- Sort the vocabulary by the frequency of words, larger the first')\n",
    "        candidate_vocab = sorted(candidate_vocab, key=lambda x: -x[1])\n",
    "\n",
    "    vocab = [w for w, _ in candidate_vocab]\n",
    "\n",
    "    assert len(vocab) == len(counts) - len(bad_words)\n",
    "\n",
    "    print('- Top 100 words:')\n",
    "    print(vocab[:100])\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37c38e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_threshold = 2\n",
    "sort_vocab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f62bae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The number of bad words: 4723/8204 = 57.57%\n",
      "- The number of the vocabulary: 3481\n",
      "- The number of UNKs: 5931/154695 = 3.83%\n",
      "- Sort the vocabulary by the frequency of words, larger the first\n",
      "- Top 100 words:\n",
      "['a', 'is', 'the', 'in', 'man', 'and', 'of', 'on', 'to', 'woman', 'are', 'with', 'about', 'talking', 'video', 'person', 'playing', 'game', 'people', 'an', 'two', 'girl', 'some', 'for', 'men', 'from', 'car', 'his', 'while', 'at', 'showing', 'show', 'cartoon', 'her', 'being', 'singing', 'someone', 'how', 'group', 'talks', 'stage', 'it', 'there', 'shown', 'movie', 'other', 'black', 'young', 's', 'food', 'dancing', 'women', 'music', 'guy', 'by', 'something', 'into', 'cooking', 'walking', 'white', 'song', 'screen', 'around', 'down', 'another', 'boy', 'up', 'wearing', 'clip', 'as', 'minecraft', 'giving', 'kitchen', 'lady', 'sitting', 'speaking', 'scene', 'that', 'their', 'camera', 'one', 'shows', 'out', 'each', 'plays', 'red', 'then', 'tv', 'he', 'animated', 'through', 'characters', 'different', 'blue', 'explaining', 'news', 'band', 'this', 'standing', 'character']\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(raw_caps_train, word_count_threshold, sort_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb935d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions_and_pos_tags(raw_caps_all, vocab):\n",
    "    itow = {i + 6: w for i, w in enumerate(vocab)}\n",
    "    itow[Constants.PAD] = Constants.PAD_WORD\n",
    "    itow[Constants.UNK] = Constants.UNK_WORD\n",
    "    itow[Constants.BOS] = Constants.BOS_WORD\n",
    "    itow[Constants.EOS] = Constants.EOS_WORD\n",
    "    itow[Constants.MASK] = Constants.MASK_WORD\n",
    "    itow[Constants.VIS] = Constants.VIS_WORD\n",
    "\n",
    "    wtoi = {w: i for i, w in itow.items()}  # inverse table\n",
    "\n",
    "    ptoi = {}\n",
    "    ptoi[Constants.PAD_WORD] = Constants.PAD\n",
    "    ptoi[Constants.UNK_WORD] = Constants.UNK\n",
    "    ptoi[Constants.BOS_WORD] = Constants.BOS\n",
    "    ptoi[Constants.EOS_WORD] = Constants.EOS\n",
    "    ptoi[Constants.MASK_WORD] = Constants.MASK\n",
    "    ptoi[Constants.VIS_WORD] = Constants.VIS\n",
    "    tag_start_i = 6\n",
    "\n",
    "    captions = defaultdict(list)\n",
    "    pos_tags = defaultdict(list)\n",
    "    for vid, caps in tqdm(raw_caps_all.items()):\n",
    "        for cap in caps:\n",
    "            tag_res = nltk.pos_tag(cap)\n",
    "\n",
    "            caption_id = [Constants.BOS]\n",
    "            tagging_id = [Constants.BOS]\n",
    "\n",
    "            for w, t in zip(cap, tag_res):\n",
    "                assert t[0] == w\n",
    "                tag = Constants.pos_tag_mapping[t[1]]\n",
    "\n",
    "                if w in wtoi.keys():\n",
    "                    caption_id += [wtoi[w]]\n",
    "                    if tag not in ptoi.keys():\n",
    "                        ptoi[tag] = tag_start_i\n",
    "                        tag_start_i += 1\n",
    "                    tagging_id += [ptoi[tag]]\n",
    "                else:\n",
    "                    caption_id += [Constants.UNK]\n",
    "                    tagging_id += [Constants.UNK]\n",
    "\n",
    "            caption_id += [Constants.EOS]\n",
    "            tagging_id += [Constants.EOS]\n",
    "\n",
    "            captions[vid].append(caption_id)\n",
    "            pos_tags[vid].append(tagging_id)\n",
    "\n",
    "    itop = {i: t for t, i in ptoi.items()}\n",
    "    return itow, captions, itop, pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dfeffda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:22<00:00, 44.30it/s]\n"
     ]
    }
   ],
   "source": [
    "itow, captions, itop, pos_tags = get_captions_and_pos_tags(raw_caps_all, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7f37192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_info(captions):\n",
    "    length_info = {}\n",
    "    max_length = 50\n",
    "\n",
    "    for vid, caps in captions.items():\n",
    "        length_info[vid] = [0] * max_length\n",
    "        for cap in caps:\n",
    "            length = len(cap) - 2 # exclude <bos>, <eos>\n",
    "            if length >= max_length:\n",
    "                continue\n",
    "            length_info[vid][length] += 1\n",
    "\n",
    "    return length_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "908f9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_info = get_length_info(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "43e1a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "        'split': split,                # {'train': [0, 1, 2, ...], 'validate': [...], 'test': [...]}\n",
    "        'vid2id': vid2id,\n",
    "        'split_category': split_category,\n",
    "        'itoc': itoc,\n",
    "        'itow': itow,                       # id to word\n",
    "        'itop': itop,                       # id to POS tag\n",
    "        'length_info': length_info,         # id to length info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f72cc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({\n",
    "            'info': info,\n",
    "            'captions': captions,\n",
    "            'pos_tags': pos_tags,\n",
    "            }, \n",
    "            open(\"MSRVTT/info_corpus.pkl\", 'wb')\n",
    ")\n",
    "\n",
    "if references is not None:\n",
    "    pickle.dump(\n",
    "        references,\n",
    "        open(\"MSRVTT/refs.pkl\", 'wb')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce4bd8",
   "metadata": {},
   "source": [
    "# Check The Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0fcf627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"MSRVTT/info_corpus.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64f6efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = data['captions']\n",
    "pos_tags = data['pos_tags']\n",
    "\n",
    "info = data['info']    \n",
    "itow = info['itow']\n",
    "itoc = info.get('itoc', None)        \n",
    "itop = info.get('itop', None)\n",
    "length_info = info['length_info']\n",
    "splits = info['split']\n",
    "split_category = info.get('split_category', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dee3c433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions['video1000']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
